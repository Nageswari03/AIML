{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_Project_Sarcasm_Detection_Nageswari_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pp68FAQf9aMN"
      },
      "source": [
        "# Sarcasm Detection\n",
        " **Acknowledgement**\n",
        "\n",
        "Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019).\n",
        "\n",
        "**Required Files given in below link.**\n",
        "\n",
        "https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S3Wj_mIZ8S3K"
      },
      "source": [
        "## Install `Tensorflow2.0` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jW2Uk8otQvi8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "1c7d99ce-9329-46a8-d084-6afe5424e875"
      },
      "source": [
        "!!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 58.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (45.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v9kv9tyJ77eF"
      },
      "source": [
        "## Get Required Files from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D0O_n6OIEVyL",
        "outputId": "9f9b48ef-2d5b-4cac-8fe9-27feab9acb6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0mgRpOvFMjKR",
        "colab": {}
      },
      "source": [
        "#Set your project path \n",
        "project_path =  '/content/drive/My Drive/NLP/' \n",
        "data_file = project_path + 'Data-20200229T003531Z-001/Data/Sarcasm_Headlines_Dataset.json'## Add your path here ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WXYwajPeQbRq"
      },
      "source": [
        "#**## Reading and Exploring Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vAk6BRUh8CqL"
      },
      "source": [
        "## Read Data \"Sarcasm_Headlines_Dataset.json\". Explore the data and get  some insights about the data. ( 8 marks)\n",
        "Hint - As its in json format you need to use pandas.read_json function. Give paraemeter lines = True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "StSLB-T8PuGr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "1b8e3196-01bc-42d1-c5d8-d07518149784"
      },
      "source": [
        "import pandas as pd\n",
        "data_json = pd.read_json(data_file, typ = 'frame', lines = True)\n",
        "\n",
        "data_json.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/advancing...</td>\n",
              "      <td>advancing the world's women</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/how-meat-...</td>\n",
              "      <td>the fascinating case for eating lab-grown meat</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/boxed-col...</td>\n",
              "      <td>this ceo will send your kids to school, if you...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>https://politics.theonion.com/top-snake-handle...</td>\n",
              "      <td>top snake handler leaves sinking huckabee camp...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/fridays-m...</td>\n",
              "      <td>friday's morning email: inside trump's presser...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        article_link  ... is_sarcastic\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...  ...            0\n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...  ...            0\n",
              "2  https://local.theonion.com/mom-starting-to-fea...  ...            1\n",
              "3  https://politics.theonion.com/boehner-just-wan...  ...            1\n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...  ...            0\n",
              "5  https://www.huffingtonpost.com/entry/advancing...  ...            0\n",
              "6  https://www.huffingtonpost.com/entry/how-meat-...  ...            0\n",
              "7  https://www.huffingtonpost.com/entry/boxed-col...  ...            0\n",
              "8  https://politics.theonion.com/top-snake-handle...  ...            1\n",
              "9  https://www.huffingtonpost.com/entry/fridays-m...  ...            0\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGlo9HEQlGmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad2755da-3edc-4be1-b0de-8dcff79cfe0f"
      },
      "source": [
        "data_json.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26709, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXQCf0z7s9Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_json.describe().transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAmTBDxxlKMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "f76ca59b-fbcd-42ae-b3d4-2c5530b04143"
      },
      "source": [
        "data_json.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26709 entries, 0 to 26708\n",
            "Data columns (total 3 columns):\n",
            "article_link    26709 non-null object\n",
            "headline        26709 non-null object\n",
            "is_sarcastic    26709 non-null int64\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 626.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7uShMaPmZ1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "78f3b0e7-2f7f-406f-81c7-2e32cac956ed"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.countplot('is_sarcastic', data = data_json)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcb45ad1898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEHCAYAAABvHnsJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z6pXf7A78E2H"
      },
      "source": [
        "## Drop `article_link` from dataset. ( 4 marks)\n",
        "As we only need headline text data and is_sarcastic column for this project. We can drop artical link column here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VLSVsvrlP9qD",
        "colab": {}
      },
      "source": [
        "df = data_json.drop('article_link', axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0h6IOxU8OdH"
      },
      "source": [
        "## Get the Length of each line and find the maximum length. ( 8 marks)\n",
        "As different lines are of different length. We need to pad the our sequences using the max length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BRAsChZAQmr3",
        "colab": {}
      },
      "source": [
        "textLen = []\n",
        "\n",
        "for headline in df['headline']:\n",
        "  textLen.append(len(headline))\n",
        "\n",
        "max_len = max(textLen)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9rcCxiNwq2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c11ac7d1-f311-43ff-df40-d2650969ac9c"
      },
      "source": [
        "df['headline'] = [str(i).rjust(max_len, ' ') for i in df['headline']] \n",
        "\n",
        "print(df['headline'][0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                former versace store clerk sues over secret 'black code' for minority shoppers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VPPd0YuPXi2M"
      },
      "source": [
        "#**## Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "35abKfRx8as3"
      },
      "source": [
        "## Import required modules required for modelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DVel73hYEV4r",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model, Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ziybaD1RdD9"
      },
      "source": [
        "## Set Different Parameters for the model. ( 4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jPw9gAN_EV6m",
        "colab": {}
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = max_len## Add your max length here ##\n",
        "embedding_size = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9abSe-bM8fn9"
      },
      "source": [
        "## Apply Keras Tokenizer of headline column of your data.  ( 8 marks)\n",
        "Hint - First create a tokenizer instance using Tokenizer(num_words=max_features) \n",
        "And then fit this tokenizer instance on your data column df['headline'] using .fit_on_texts()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T9Ad26HfTFMS",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7_IJcJMC9P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(df['headline'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Ffi63KsST3P"
      },
      "source": [
        "## Define X and y for your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wnjxBdqmSS4s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "2d88357f-df9c-4bc2-e2b4-0703599bdb9f"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df['headline'])\n",
        "X = pad_sequences(X, maxlen = maxlen)\n",
        "y = np.asarray(df['is_sarcastic'])\n",
        "\n",
        "print(\"Number of Samples:\", len(X))\n",
        "print(X[0])\n",
        "print(\"Number of Labels: \", len(y))\n",
        "print(y[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples: 26709\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0  307  678 3336 2297   47  381 2575    5\n",
            " 2576 8433]\n",
            "Number of Labels:  26709\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJLyKg-98rH_"
      },
      "source": [
        "## Get the Vocabulary size ( 4 marks)\n",
        "Hint : You can use tokenizer.word_index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q-2w0gHEUUIo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0681d9b9-299c-46a0-dd4f-accc159ee508"
      },
      "source": [
        "num_words = len(tokenizer.word_index)\n",
        "\n",
        "print(num_words)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5hjeMi40XcB1"
      },
      "source": [
        "#**## Word Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bUF1TuQa8ux0"
      },
      "source": [
        "## Get Glove Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vq5AIfRtMeZh",
        "colab": {}
      },
      "source": [
        "glove_file = project_path + \"Data-20200229T003531Z-001/Data/glove.6B.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DJLX_n2WMecA",
        "colab": {}
      },
      "source": [
        "#Extract Glove embedding zip file\n",
        "from zipfile import ZipFile\n",
        "with ZipFile(glove_file, 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9IuXlu8-U3HG"
      },
      "source": [
        "# Get the Word Embeddings using Embedding file as given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elZ-T5aFGZmZ",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = './glove.6B.200d.txt'\n",
        "\n",
        "embeddings = {}\n",
        "for o in open(EMBEDDING_FILE):\n",
        "    word = o.split(\" \")[0]\n",
        "    # print(word)\n",
        "    embd = o.split(\" \")[1:]\n",
        "    embd = np.asarray(embd, dtype='float32')\n",
        "    # print(embd)\n",
        "    embeddings[word] = embd\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bTPxveDmVCrA"
      },
      "source": [
        "# Create a weight matrix for words in training docs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xQgOhiywU9nU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c508da11-099a-437d-a98f-ca9b52be780e"
      },
      "source": [
        "embedding_matrix = np.zeros((num_words,embedding_size))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i-1] = embedding_vector\n",
        "\n",
        "len(embeddings.values())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u7IbWuEX82Ra"
      },
      "source": [
        "## Create and Compile your Model  ( 14 marks)\n",
        "Hint - Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, then dense and dropout layers as required. \n",
        "In the end add a final dense layer with sigmoid activation for binary classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d7jhsSgYXG4l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "3223ffec-ac0e-4b84-d6ab-0666f65cacca"
      },
      "source": [
        "\n",
        "### Embedding layer for hint \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_size, weights = [embedding_matrix]))\n",
        "### Bidirectional LSTM layer for hint \n",
        "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 200)         5931200   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 256)         336896    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 64)          16448     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 32)          2080      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 32)          0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, None, 16)          528       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, None, 8)           136       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 8)           0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, None, 1)           9         \n",
            "=================================================================\n",
            "Total params: 6,287,297\n",
            "Trainable params: 6,287,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IJFMxZwMWoTw"
      },
      "source": [
        "## Fit your model with a batch size of 100 and validation_split = 0.2. and state the validation accuracy ( 10 marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZpVkajCcWnRK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3859a48f-09ea-4454-d093-9b0ca9840826"
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 5\n",
        "validation_split = 0.2\n",
        " ## Add your code here ##\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "checkpoint = ModelCheckpoint(\"model-{loss:.2f}.h5\", monitor=\"loss\", verbose=1, save_best_only=True,\n",
        "                             save_weights_only=True, mode=\"min\", period=1)\n",
        "stop = EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, patience=5, min_lr=1e-5, verbose=1, mode=\"min\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbqmvGkcN0b-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d7d6ae31-5dbb-45e8-de11-0e566d2fc400"
      },
      "source": [
        "history = model.fit(x=X, y =y, validation_split=.2, epochs = 5, batch_size = 100, verbose = 1, callbacks=[stop,checkpoint,reduce_lr], shuffle=True)\n",
        "#history = model.fit(x=X, y =y, validation_split=.2, epochs = 5, batch_size = 100, verbose = 1, shuffle=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 21367 samples, validate on 5342 samples\n",
            "Epoch 1/5\n",
            "21300/21367 [============================>.] - ETA: 1s - loss: 0.6875 - accuracy: 0.5573\n",
            "Epoch 00001: loss improved from inf to 0.68753, saving model to model-0.69.h5\n",
            "21367/21367 [==============================] - 449s 21ms/sample - loss: 0.6875 - accuracy: 0.5574 - val_loss: 0.6836 - val_accuracy: 0.5680\n",
            "Epoch 2/5\n",
            "21300/21367 [============================>.] - ETA: 1s - loss: 0.6617 - accuracy: 0.6042\n",
            "Epoch 00002: loss improved from 0.68753 to 0.66171, saving model to model-0.66.h5\n",
            "21367/21367 [==============================] - 446s 21ms/sample - loss: 0.6617 - accuracy: 0.6046 - val_loss: 0.5836 - val_accuracy: 0.7665\n",
            "Epoch 3/5\n",
            "21300/21367 [============================>.] - ETA: 1s - loss: 0.4296 - accuracy: 0.8281\n",
            "Epoch 00003: loss improved from 0.66171 to 0.42940, saving model to model-0.43.h5\n",
            "21367/21367 [==============================] - 456s 21ms/sample - loss: 0.4294 - accuracy: 0.8281 - val_loss: 0.3696 - val_accuracy: 0.8432\n",
            "Epoch 4/5\n",
            "21300/21367 [============================>.] - ETA: 1s - loss: 0.2616 - accuracy: 0.9058\n",
            "Epoch 00004: loss improved from 0.42940 to 0.26161, saving model to model-0.26.h5\n",
            "21367/21367 [==============================] - 452s 21ms/sample - loss: 0.2616 - accuracy: 0.9059 - val_loss: 0.3679 - val_accuracy: 0.8476\n",
            "Epoch 5/5\n",
            "21300/21367 [============================>.] - ETA: 1s - loss: 0.1768 - accuracy: 0.9420\n",
            "Epoch 00005: loss improved from 0.26161 to 0.17654, saving model to model-0.18.h5\n",
            "21367/21367 [==============================] - 449s 21ms/sample - loss: 0.1765 - accuracy: 0.9421 - val_loss: 0.4231 - val_accuracy: 0.8454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BY4EVvLOXUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b7feac71-6f47-47ec-c6af-0a90c70509a0"
      },
      "source": [
        "print(\"Validation Accuracy: \")\n",
        "print(history.history['val_accuracy'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: \n",
            "[0.5679521, 0.76654845, 0.8432427, 0.84755915, 0.8454081]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}